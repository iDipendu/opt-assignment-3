import numpy as np
import matplotlib.pyplot as plt

# Rosenbrock function
def f(xy):
    x, y = xy
    return (1 - x)**2 + 100 * (y - x**2)**2

# Gradient of the Rosenbrock function
def grad_f(xy):
    x, y = xy
    df_dx = -2 * (1 - x) - 400 * x * (y - x**2)
    df_dy = 200 * (y - x**2)
    return np.array([df_dx, df_dy])

# Initialization
xy = np.array([-1.0, 1.0])  # Initial point
eta = 0.001
epsilon = 1e-6
max_iter = 10000

loss_history = []
trajectory = [xy.copy()]

# Gradient descent loop
for i in range(max_iter):
    grad = grad_f(xy)
    loss = f(xy)
    loss_history.append(loss)

    if np.linalg.norm(grad) < epsilon:
        break

    xy -= eta * grad
    trajectory.append(xy.copy())

# Results
print(f"Final (x, y): {xy}")
print(f"Iterations: {i + 1}")
print(f"Final loss: {loss}")

# Plot: Loss vs Iterations
plt.figure()
plt.plot(loss_history)
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title("Convergence of Gradient Descent (Rosenbrock Function)")
plt.grid(True)
plt.show()

# Plot: Descent path on contour
trajectory = np.array(trajectory)
x_vals = np.linspace(-2, 2, 400)
y_vals = np.linspace(-1, 3, 400)
X, Y = np.meshgrid(x_vals, y_vals)
Z = (1 - X)**2 + 100 * (Y - X**2)**2

plt.figure(figsize=(8, 6))
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='jet')
plt.plot(trajectory[:, 0], trajectory[:, 1], 'ro--', label='Descent Path')
plt.xlabel("x")
plt.ylabel("y")
plt.title("Rosenbrock Function Descent Path")
plt.legend()
plt.grid(True)
plt.show()
